{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import Cleaned Dataset**"
      ],
      "metadata": {
        "id": "hjpw4JbYP-vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import json, os\n",
        "\n",
        "kaggle_json = {\n",
        "    \"username\": userdata.get('KAGGLE_USERNAME'),\n",
        "    \"key\": userdata.get('KAGGLE_KEY')\n",
        "}\n",
        "\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
        "    json.dump(kaggle_json, f)\n",
        "\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
        "\n",
        "# ğŸ“¥ Download and unzip\n",
        "!kaggle datasets download -d adithyabhaskar2511/stock-market-analysis --unzip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_h0wjufFX4B",
        "outputId": "d6b80d53-f2ad-4866-abe1-069c03c8a460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/adithyabhaskar2511/stock-market-analysis\n",
            "License(s): MIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Ready Preprocessed Data**"
      ],
      "metadata": {
        "id": "Vj1aLF6rQFEF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcegxsrEDyCe",
        "outputId": "dc7242a3-ee2e-4cb5-c5b5-921de2dad28c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš€ Preprocessing: ITC\n",
            "\n",
            "ğŸš€ Preprocessing: AAPL\n",
            "\n",
            "ğŸ“† All stocks preprocessed and saved. Models will be trained LIVE.\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“ƒ Notebook 4 â€“ Fast Preprocessing & Model-Ready Storage for 30 Stocks\n",
        "\n",
        "\"\"\"\n",
        "This notebook is optimized to:\n",
        "- Load each stock\n",
        "- Preprocess & split data\n",
        "- Store scaled and ready-to-use train/test sets in `.npz` files\n",
        "\n",
        "Model training (LSTM, GRU, Prophet) will be handled during deployment\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ğŸ“ Ensure output folders exist\n",
        "os.makedirs(\"preprocessed\", exist_ok=True)\n",
        "\n",
        "# âœ… Define 30 tickers and source paths\n",
        "ticker_info = {\n",
        "    # Static Stocks (NIFTY 50)\n",
        "\n",
        "    \"ITC\": \"static\",\n",
        "\n",
        "\n",
        "\n",
        "    # Global Stocks\n",
        "    \"AAPL\": \"live\"\n",
        "}\n",
        "\n",
        "# â© Helper to create sequences\n",
        "def create_sequences(data, seq_length=60):\n",
        "    X, y = [], []\n",
        "    for i in range(seq_length, len(data)):\n",
        "        X.append(data[i - seq_length:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# ğŸš€ Preprocessing loop\n",
        "for ticker, src in ticker_info.items():\n",
        "    print(f\"\\nğŸš€ Preprocessing: {ticker}\")\n",
        "\n",
        "    # Handle different filename patterns\n",
        "    if src == \"static\":\n",
        "        path = f\"data/processed/enriched/{src}/{ticker}_WITH_INDICATORS__clean.csv\"\n",
        "    else:\n",
        "        path = f\"data/processed/enriched/{src}/{ticker}_live.csv\"\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"âŒ Skipping (file not found): {path}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "        # Try to parse Date column\n",
        "        if \"Date\" in df.columns:\n",
        "            df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "            df = df.set_index(\"Date\")\n",
        "        elif df.columns[0].lower().startswith(\"date\"):\n",
        "            df.rename(columns={df.columns[0]: \"Date\"}, inplace=True)\n",
        "            df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "            df = df.set_index(\"Date\")\n",
        "        else:\n",
        "            print(f\"âŒ No valid 'Date' column found in {ticker}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Fix column name for Close\n",
        "        if \"close\" not in df.columns:\n",
        "            if \"unnamed: 4\" in df.columns:\n",
        "                df.rename(columns={\"unnamed: 4\": \"close\"}, inplace=True)\n",
        "\n",
        "        if \"close\" not in df.columns:\n",
        "            print(f\"âŒ 'close' column missing in {ticker} even after renaming. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        df = df[[\"close\"]].dropna()\n",
        "\n",
        "        # Scale data\n",
        "        scaler = MinMaxScaler()\n",
        "        scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "        train_size = int(len(scaled_data) * 0.8)\n",
        "        train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]\n",
        "\n",
        "        X_train, y_train = create_sequences(train_data)\n",
        "        X_test, y_test = create_sequences(test_data)\n",
        "\n",
        "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "        # Save preprocessed data\n",
        "        np.savez_compressed(f\"preprocessed/{ticker}_processed.npz\",\n",
        "                            X_train=X_train, y_train=y_train,\n",
        "                            X_test=X_test, y_test=y_test,\n",
        "                            scaler_min=scaler.data_min_,\n",
        "                            scaler_max=scaler.data_max_)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error processing {ticker}: {e}\")\n",
        "\n",
        "print(\"\\nğŸ“† All stocks preprocessed and saved. Models will be trained LIVE.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cmdstanpy\n",
        "!pip install numpy pandas matplotlib scikit-learn tensorflow prophet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbHWx3wtRU-9",
        "outputId": "0cc2b6c8-d3c9-4d75-ad64-240c3739d161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cmdstanpy\n",
            "  Downloading cmdstanpy-1.2.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from cmdstanpy) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from cmdstanpy) (4.67.1)\n",
            "Collecting stanio<2.0.0,>=0.4.0 (from cmdstanpy)\n",
            "  Downloading stanio-0.5.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->cmdstanpy) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->cmdstanpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->cmdstanpy) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->cmdstanpy) (1.17.0)\n",
            "Downloading cmdstanpy-1.2.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stanio-0.5.1-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: stanio, cmdstanpy\n",
            "Successfully installed cmdstanpy-1.2.5 stanio-0.5.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting prophet\n",
            "  Downloading prophet-1.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (1.2.5)\n",
            "Collecting holidays<1,>=0.25 (from prophet)\n",
            "  Downloading holidays-0.71-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.11/dist-packages (from prophet) (4.67.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from prophet) (6.5.2)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prophet-1.1.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading holidays-0.71-py3-none-any.whl (917 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m917.9/917.9 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, holidays, astunparse, tensorflow, prophet\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 holidays-0.71 libclang-18.1.1 prophet-1.1.6 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training And Comparison**"
      ],
      "metadata": {
        "id": "laJIDPGUQMmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“’ Notebook 4 â€“ Fast Model Training & Comparison for All Stocks\n",
        "\n",
        "\"\"\"\n",
        "- Loads preprocessed data from /preprocessed\n",
        "- Trains LSTM, GRU, Prophet models for each ticker\n",
        "- Saves trained models to /models\n",
        "- Generates comparison plots in /plots\n",
        "- Uses fewer epochs for faster execution\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
        "from prophet import Prophet\n",
        "from math import sqrt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "os.makedirs(\"plots\", exist_ok=True)\n",
        "\n",
        "# âœ… Auto-detect preprocessed tickers\n",
        "tickers = [f.split(\"_\")[0] for f in os.listdir(\"preprocessed\") if f.endswith(\".npz\")]\n",
        "\n",
        "# ğŸš€ Model training loop\n",
        "for ticker in tickers:\n",
        "    print(f\"\\nğŸš€ Training Models for: {ticker}\")\n",
        "\n",
        "    try:\n",
        "        # Load preprocessed data\n",
        "        data = np.load(f\"preprocessed/{ticker}_processed.npz\")\n",
        "        X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
        "        X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
        "        min_, max_ = data[\"scaler_min\"], data[\"scaler_max\"]\n",
        "\n",
        "        def rescale(y):\n",
        "            return y * (max_ - min_) + min_\n",
        "\n",
        "        y_true = rescale(y_test.reshape(-1, 1))\n",
        "\n",
        "        # --- LSTM ---\n",
        "        lstm_model = Sequential([\n",
        "            LSTM(32, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "            LSTM(16),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        lstm_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
        "        lstm_model.fit(X_train, y_train, epochs=3, batch_size=32, verbose=0)\n",
        "        lstm_preds = lstm_model.predict(X_test)\n",
        "        lstm_preds_rescaled = rescale(lstm_preds)\n",
        "        lstm_rmse = sqrt(mean_squared_error(y_true, lstm_preds_rescaled))\n",
        "        lstm_model.save(f\"models/lstm_{ticker}.h5\")\n",
        "\n",
        "        # --- GRU ---\n",
        "        gru_model = Sequential([\n",
        "            GRU(32, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
        "            GRU(16),\n",
        "            Dense(1)\n",
        "        ])\n",
        "        gru_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
        "        gru_model.fit(X_train, y_train, epochs=3, batch_size=32, verbose=0)\n",
        "        gru_preds = gru_model.predict(X_test)\n",
        "        gru_preds_rescaled = rescale(gru_preds)\n",
        "        gru_rmse = sqrt(mean_squared_error(y_true, gru_preds_rescaled))\n",
        "        gru_model.save(f\"models/gru_{ticker}.h5\")\n",
        "\n",
        "        # --- Prophet ---\n",
        "        prophet_rmse = None\n",
        "        forecast_close = None\n",
        "        try:\n",
        "            # Guess source folder\n",
        "            src_folder = \"static\" if not ticker.endswith(\"_NS\") and ticker.isupper() else \"live\"\n",
        "            p1 = f\"data/processed/enriched/{src_folder}/{ticker}_WITH_INDICATORS__clean.csv\"\n",
        "            p2 = f\"data/processed/enriched/{src_folder}/{ticker}_live.csv\"\n",
        "            path = p1 if os.path.exists(p1) else p2\n",
        "\n",
        "            df = pd.read_csv(path)\n",
        "            if \"unnamed: 4\" in df.columns:\n",
        "                df.rename(columns={\"unnamed: 4\": \"close\"}, inplace=True)\n",
        "            if \"Date\" not in df.columns:\n",
        "                df.reset_index(inplace=True)\n",
        "                if \"index\" in df.columns:\n",
        "                    df.rename(columns={\"index\": \"Date\"}, inplace=True)\n",
        "\n",
        "            df = df[[\"Date\", \"close\"]].dropna()\n",
        "            df.rename(columns={\"Date\": \"ds\", \"close\": \"y\"}, inplace=True)\n",
        "            df[\"ds\"] = pd.to_datetime(df[\"ds\"], errors=\"coerce\")\n",
        "            df.dropna(subset=[\"ds\", \"y\"], inplace=True)\n",
        "            if df[\"ds\"].dt.tz is not None:\n",
        "                df[\"ds\"] = df[\"ds\"].dt.tz_localize(None)\n",
        "\n",
        "            prophet_train = df.iloc[:int(len(df) * 0.8)]\n",
        "            prophet_model = Prophet()\n",
        "            prophet_model.fit(prophet_train)\n",
        "            future = prophet_model.make_future_dataframe(periods=len(df) - len(prophet_train))\n",
        "            forecast = prophet_model.predict(future)\n",
        "            forecast_close = forecast[[\"yhat\"]].values[-len(y_true):]\n",
        "            prophet_rmse = sqrt(mean_squared_error(y_true, forecast_close))\n",
        "            forecast.to_csv(f\"models/prophet_{ticker}.csv\", index=False)\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸ Prophet failed for {ticker}: {e}\")\n",
        "\n",
        "        # Plot results\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(y_true, label=\"Actual\", color=\"black\")\n",
        "        plt.plot(lstm_preds_rescaled, label=\"LSTM\", alpha=0.6)\n",
        "        plt.plot(gru_preds_rescaled, label=\"GRU\", alpha=0.6)\n",
        "        if forecast_close is not None:\n",
        "            plt.plot(forecast_close, label=\"Prophet\", alpha=0.6)\n",
        "        plt.title(f\"Comparison â€“ {ticker}\")\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"plots/{ticker}_comparison.png\")\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"  âœ… Done: LSTM RMSE={lstm_rmse:.2f}, GRU RMSE={gru_rmse:.2f}, Prophet RMSE={prophet_rmse if prophet_rmse else 'N/A'}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ Failed on {ticker}: {e}\")\n",
        "\n",
        "print(\"\\nğŸ† All models trained and plots saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnze0P12P9Sh",
        "outputId": "90bb36bc-1a0a-49f2-bcae-d9a4495c7413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš€ Training Models for: ITC\n",
            "\u001b[1m813/813\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m813/813\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpbp9blb_d/rebewhgg.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpbp9blb_d/_x6kkore.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=18474', 'data', 'file=/tmp/tmpbp9blb_d/rebewhgg.json', 'init=/tmp/tmpbp9blb_d/_x6kkore.json', 'output', 'file=/tmp/tmpbp9blb_d/prophet_model2dp2z94q/prophet_model-20250502122857.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "12:28:57 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "12:34:26 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  âœ… Done: LSTM RMSE=0.52, GRU RMSE=0.40, Prophet RMSE=2465.4261586426346\n",
            "\n",
            "ğŸš€ Training Models for: AAPL\n",
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:tensorflow:5 out of the last 822 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7d5a30554860> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  âš ï¸ Prophet failed for AAPL: [Errno 2] No such file or directory: 'data/processed/enriched/static/AAPL_live.csv'\n",
            "  âœ… Done: LSTM RMSE=9.32, GRU RMSE=6.10, Prophet RMSE=N/A\n",
            "\n",
            "ğŸ† All models trained and plots saved.\n"
          ]
        }
      ]
    }
  ]
}